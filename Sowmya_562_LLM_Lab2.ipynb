{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNTuxeZ1hwsjib30urkGdg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sowmya-2348562/LLM/blob/main/Sowmya_562_LLM_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWVf52no37yT",
        "outputId": "a69ee98b-e1fe-46b3-fbb8-01f4460ec675"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "5zV63Ttk4Fkm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)  # Get the most likely beginning of answer\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "y5VS7Jlg4Jbh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Paris is the capital and most populous city of France. The city is a major European hub for art, fashion, and culture.\"\n",
        "question = \"What is the capital of France?\"\n",
        "answer = answer_question(question, context)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJvCTxBF4N9A",
        "outputId": "011a5dc7-2072-49bf-f5d4-19986dbade51"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of France?\n",
            "Answer: paris\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Different prompts to test the impact on the model output\n",
        "prompts = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Can you tell me the capital city of France?\",\n",
        "    \"Which city is the capital of France?\"\n",
        "]\n",
        "\n",
        "context = \"Paris is the capital and most populous city of France. The city is a major European hub for art, fashion, and culture.\"\n",
        "\n",
        "# Function to test different prompts\n",
        "def test_prompts(prompts, context):\n",
        "    results = {}\n",
        "    for prompt in prompts:\n",
        "        answer = answer_question(prompt, context)\n",
        "        results[prompt] = answer\n",
        "    return results\n",
        "\n",
        "# Test the prompts\n",
        "results = test_prompts(prompts, context)\n",
        "for prompt, answer in results.items():\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Answer: {answer}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uRoTpxf4RJY",
        "outputId": "63cfe9a9-e79a-46af-a380-5bf3715b1910"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What is the capital of France?\n",
            "Answer: paris\n",
            "\n",
            "Prompt: Can you tell me the capital city of France?\n",
            "Answer: paris\n",
            "\n",
            "Prompt: Which city is the capital of France?\n",
            "Answer: paris\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Engineering Insights\n",
        "\n",
        "**Prompt 1:** \"What is the capital of France?\"\n",
        "**Output:** \"Paris\"\n",
        "\n",
        "**Prompt 2:** \"Can you tell me the capital city of France?\"\n",
        "**Output:** \"Paris\"\n",
        "\n",
        "**Prompt 3:** \"Which city is the capital of France?\"\n",
        "**Output:** \"Paris\"\n"
      ],
      "metadata": {
        "id": "8N7aVtQO4ZTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"distilbert-base-uncased-distilled-squad\"  # You can choose other models as well\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "# Function to answer questions\n",
        "def answer_question(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "# Define the context\n",
        "contexts = {\n",
        "    \"largest_planet\": \"Jupiter is the largest planet in our solar system.\",\n",
        "    \"relativity\": \"The theory of relativity, developed by Albert Einstein, revolutionized our understanding of space, time, and gravity.\",\n",
        "    \"mockingbird\": \"'To Kill a Mockingbird' is a novel by Harper Lee.\"\n",
        "}\n",
        "\n",
        "# Define the prompts\n",
        "prompts = {\n",
        "    \"largest_planet\": \"What is the largest planet in our solar system?\",\n",
        "    \"relativity\": \"Explain the theory of relativity in simple terms.\",\n",
        "    \"mockingbird\": \"Who wrote 'To Kill a Mockingbird'?\"\n",
        "}\n",
        "\n",
        "# Get responses from your model\n",
        "responses = {}\n",
        "for key in prompts:\n",
        "    responses[key] = answer_question(prompts[key], contexts[key])\n",
        "\n",
        "# Print the responses\n",
        "for key in prompts:\n",
        "    print(f\"Prompt: {prompts[key]}\")\n",
        "    print(f\"Answer: {responses[key]}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaYRCBWA4VJq",
        "outputId": "ec149499-bd3b-482f-95ac-c98ab0e2fbe1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: What is the largest planet in our solar system?\n",
            "Answer: jupiter\n",
            "\n",
            "Prompt: Explain the theory of relativity in simple terms.\n",
            "Answer: the theory of relativity, developed by albert einstein, revolutionized our understanding of space, time, and gravity\n",
            "\n",
            "Prompt: Who wrote 'To Kill a Mockingbird'?\n",
            "Answer: harper lee\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL - CHATGPT 3.5\n",
        "\n",
        "1. **What is the largest planet in our solar system?**\n",
        "   - The largest planet in our solar system is Jupiter.\n",
        "\n",
        "2. **Explain the theory of relativity in simple terms:**\n",
        "   - The theory of relativity, developed by Albert Einstein, is a fundamental theory in physics that explains how space, time, and gravity work together. It has two main parts:\n",
        "     - **Special Relativity:** Deals with objects moving at constant speeds relative to each other and describes how time and space are affected.\n",
        "     - **General Relativity:** Describes gravity as the curvature of space and time caused by mass and energy.\n",
        "\n",
        "3. **Who wrote 'To Kill a Mockingbird'?**\n",
        "   - 'To Kill a Mockingbird' is a novel written by Harper Lee."
      ],
      "metadata": {
        "id": "e-od2zss8hFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL 2 : Gemini AI :\n",
        "\n",
        "The largest planet in our solar system is Jupiter.\n",
        "\n",
        "\n",
        "The theory of relativity is a complex concept, but in simple terms, it suggests that space and time are not fixed but are relative to the observer and the influence of gravity. Imagine a trampoline - a heavy object like a bowling ball will warp the fabric of the trampoline more than a marble, similar to how massive objects like stars warp spacetime.\n",
        "\n",
        "\n",
        "'To Kill a Mockingbird' was written by Harper Lee."
      ],
      "metadata": {
        "id": "Hhs0u7ga-Xar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferences\n",
        "Prompt Engineering and Optimization:\n",
        "\n",
        "Prompt Definition: The way prompts are defined plays a crucial role in determining the quality and relevance of the model's responses. Optimizing prompts involves crafting them in a way that effectively guides the model towards generating accurate and informative answers.\n",
        "Impact of Prompt Formulation: Different formulations of prompts can lead to varied outputs from the models. For example, a more specific prompt might elicit a more detailed response, while a broader prompt might yield a general answer. Understanding this impact helps in refining prompts to suit specific use cases or query types."
      ],
      "metadata": {
        "id": "Ds30Twn-Bcpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "inference:\n",
        "All models correctly identify Jupiter as the largest planet in our solar system, indicating strong factual knowledge.\n",
        "\n",
        "ChatGPT 3.5 provides a more detailed explanation of the theory of relativity compared to My Model and Gemini AI, breaking it down into its components and offering a comprehensive understanding.\n",
        "\n",
        "ChatGPT 3.5 excels in natural language generation, presenting complex ideas like the theory of relativity in an engaging and understandable manner.\n",
        "\n",
        "All models consistently attribute 'To Kill a Mockingbird' to Harper Lee, demonstrating accurate factual retrieval and contextual understanding.\n",
        "\n",
        "The impact of prompt formulation is evident, with more specific prompts leading to more detailed responses.\n",
        "\n",
        "ChatGPT 3.5's ability to provide detailed and natural explanations makes it suitable for educational and informational purposes where clarity and depth are important.\n",
        "\n",
        "While all models perform well in factual retrieval, ChatGPT 3.5 stands out in providing detailed and natural explanations, showcasing strength in handling nuanced prompts and delivering engaging responses suitable for a broad range of applications."
      ],
      "metadata": {
        "id": "3SNUP7fqCin-"
      }
    }
  ]
}